Proposed Architecture: Multi-Stage Attention-Enhanced Hybrid Transformer Network (MAHT-Net)

MAHT-Net: Multi-Stage Attention-enhanced Hybrid Transformer Network for Cephalometric Landmark Detection
 
MAHT-Net combines the following in it architecture:
•	Self-Attention (Transformer) blocks for long-range spatial dependencies,
•	U-Net for its proven effectiveness in medical image segmentation,
•	Multi-Scale Feature Fusion to enhance landmark localization,
•	Heatmap based point regression for precise landmark detection.

Architecture Components

1. Overview
To overcome the limitations of traditional U-Net architectures and achieve robust, precise, and clinically acceptable cephalometric point detection, we propose MAHT-Net (Multi-stage Attention-enhanced Hybrid Transformer Network). This architecture integrates convolutional feature extraction, attention-based spatial encoding, and heatmap-based landmark regression in a multi-scale, end-to-end learnable framework.

2. Network Architecture (Key Components of MAHT-Net)
2.1 Encoder: Pretrained EfficientNet or ResNet-34
The encoder is responsible for extracting hierarchical features from the input cephalometric radiographs. Instead of using a standard U-Net encoder, we integrate a pretrained EfficientNet-B3 or ResNet-34 model, which enhances low-level feature richness and facilitates transfer learning. These models offers a balance between accuracy and computational efficiency, while ResNet-34 is known for its deep feature extraction capabilities.
Main usage in the model:
•	Pretrained on ImageNet
•	Extracts rich, hierarchical features
•	Replaces vanilla U-Net encoder for better representation
References:
EfficientNet has been utilized in various medical imaging tasks due to its performance and efficiency. ResNet-34 has been employed in cephalometric landmark detection tasks. 
Ex: https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04167.pdf

2.2 Transformer Bottleneck
To capture global anatomical dependencies and improve spatial reasoning, we integrate a Vision Transformer (ViT) or Swin Transformer bottleneck between the encoder and decoder. This enables the model to learn contextual relationships between distant cephalometric points, improving landmark consistency.
	Main usage in the model:
•	Insert Vision Transformer (ViT) or Swin Transformer module between encoder and decoder
•	Captures global context and inter-point spatial dependencies (e.g., symmetry of facial landmarks)

References:
A survey on Vision Transformers in medical imaging highlights their effectiveness. High-Resolution Swin Transformer Network (HRSTNet) has been proposed for medical image segmentation. 
https://www.mdpi.com/2076-3417/14/7/2741
https://arxiv.org/pdf/2207.11553

2.3 Attention-Gated Decoder
The decoder mirrors the encoder structure but includes Attention Gates at each upsampling stage. These gates filter irrelevant information and allow the model to focus on important cephalometric features by learning spatial attention maps. Attention, in the context of image segmentation, is a way to highlight only the relevant activations during training. This reduces the computational resources wasted on irrelevant activations, providing the network with better generalisation power. Essentially, the network can pay “attention” to certain parts of the image.

Main usage in the model:
•	Integrate Attention Gates (from Attention U-Net) at each decoder level
•	Suppresses irrelevant background noise
•	Focuses model on cephalometric regions

Some References:
https://pmc.ncbi.nlm.nih.gov/articles/PMC11300777/
https://arxiv.org/abs/1804.03999

2.4 Multi-Scale Feature Fusion
Inspired by Feature Pyramid Networks (FPN), the decoder aggregates feature maps at multiple scales using lateral skip connections. This fusion helps in detecting cephalometric points of varying sizes and enhances robustness across age groups and anatomical variability.
Main usage in the model:
•	Use Feature Pyramid Network (FPN) style lateral connections
•	Helps with landmarks of varying sizes and scales

References:
Effective medical image detection using feature pyramid fusion enhancement.
https://dl.acm.org/doi/10.1016/j.compbiomed.2023.107149
https://www.researchgate.net/publication/371471743_EFPN_Effective_medical_image_detection_using_feature_pyramid_fusion_enhancement

2.5 Heatmap Regression Output
Instead of direct coordinate regression, MAHT-Net outputs a heatmap per landmark, where each heatmap is a 2D Gaussian centered on the predicted landmark. This allows for smoother, more interpretable, and clinically accurate localization.
Main usage in the model:
•	Predicts a heatmap per landmark
•	Each heatmap is a 2D Gaussian centered on the expected landmark
•	More robust to localization noise than direct coordinate regression

References:
Integrating spatial configuration into heatmap regression based anatomical landmark localization. 
https://www.sciencedirect.com/science/article/pii/S1361841518305784

Training Pipeline

3.1 Dataset Preparation
•	Cephalometric radiographs are normalized and resized.
•	Manual annotations (X, Y) for each landmark are converted into 2D Gaussian heatmaps.
•	Extensive data augmentation is applied: rotation, flipping, scaling, brightness/contrast and elastic deformation.

3.2 Loss Function
We employ a multi-component loss:
•	Heatmap MSE Loss: Mean squared error between predicted and ground truth heatmaps.
•	SSIM Loss: To preserve structural integrity.
•	Auxiliary Classification Loss: For gender, skeletal class, or growth stage if applicable.

3.3 Optimization
•	Optimizer: Adam
•	Learning Rate Scheduler: Cosine annealing or Fixed learning rate

4. Evaluation Metrics
•	Detection Accuracy @2mm: Percentage of landmarks predicted within 2mm of ground truth.
•	Mean Radial Error (MRE): Euclidean distance between predicted and actual points.
•	Success Detection Rate (SDR) at 2mm, 2.5mm, and 3mm thresholds.
•	Visualization: Overlay of predicted heatmaps on radiographs for interpretability.

5. Clinical Integration
The trained MAHT-Net will be deployed in a lightweight interface for orthodontists. Users can upload cephalometric radiographs and receive:
 
Example of platform: https://www.cephio.com/en/

•	Automatic landmark annotations
•	Skeletal and growth stage classification
•	Measurement values (distances, angles)
•	Visual explanations via attention maps

Conceptual Diagram Description of MAHT-Net

This architecture is designed to process a cephalometric radiograph (SR) image and output precise locations of specific cephalometric points in the form of heatmaps.
1.	Input Layer:

The initial layer receives the raw cephalometric radiograph image data. The image is first resized and normalised before being fed into the network. Image transformations like rotation, mirroring, enlarging or reducing the size are performed to augment the data and then pass them as inputs for training to improve performance and reduce overfitting.
2.	Encoder (Part of Hybrid U-Net Structure):

This is similar to the downsampling path of the U-Net structure. It consists of multiple convolutional and pooling layers. Each stage progressively reduces the spatial dimensions of the feature maps while increasing the number of feature channels, effectively extracting hierarchical features from the image, from simple edges to more complex patterns. 
3.	Attention-Enhanced Blocks:

These blocks, potentially incorporating Self-Attention mechanisms to those in Transformer models, are integrated within or between the traditional convolutional pooling layers of the Encoder, Decoder, or even within the Skip Connections. Their purpose is to enable the network to capture long-range spatial dependencies across the entire image. This is crucial for locating landmarks that might be defined by their relationship to distant anatomical structures. 
4.	Fusion Step (Multi-scale Feature Fusion):

These connections link feature maps from the Encoder path directly to the Decoder path at corresponding resolution levels. This fusion allows the Decoder to utilise both high-level semantic features from the deep layers of the Encoder and low-level detailed spatial information from the earlier layers. The fusion connection layers are an integral part of the U-Net architecture, designed to overcome the "bottleneck problem" and preserve local details lost during downsampling. The project plans to use a "multi-scale object detection" approach, specifically mentioning the collective use of "U-Net models at different depths".
5.	Decoder (Part of Hybrid U-Net Structure):

This is the upsampling path of the U-Net structure. It consists of layers that gradually increase the spatial dimensions of the feature maps back towards the original input image size. It processes the features refined by the Encoder and enhanced by the Skip Connections (and potentially Attention) to reconstruct the output representation. 
6.	Output Layer (Heatmap-based Point Regression):

The final layer (or series of layers) of the network outputs a heatmap (or multiple heatmaps, one for each landmark). A heatmap is a probability distribution indicating the likelihood of a specific landmark being present at each pixel location in the image. The peak of the probability distribution on the heatmap typically corresponds to the detected location of the landmark.
7.	Post-processing (Inferring Coordinates):

An external step where the coordinates of the detected landmarks are extracted from the heatmaps will be included. This might involve finding the pixel with the maximum probability value in each heatmap or using more sophisticated techniques to refine the peak location. This step is necessary to get the final X and Y coordinates of the points, which are then used for automated cephalometric analyses (Gender Classification, Growth Stage Determination, Skeletal Classification).

Training Enhancements
•	Loss function: Weighted combination of:
o	Mean Squared Error (MSE) on heatmaps
o	Structural Similarity Index Measure (SSIM) loss
o	Auxiliary classification losses (e.g., growth stage)

•	Data augmentation:
o	Random affine transforms, Gaussian blur, brightness shifts
o	CutMix and MixUp for robustness

•	Uncertainty estimation:
o	Add Bayesian dropout or MC dropout for confidence estimation in predictions

How does MAHT-Net intersect with U-Net Architecture

1. Encoder–Decoder Symmetry (Core U-Net Principle)
•	Just like U-Net, MAHT-Net has:
o	A contracting path (the encoder — EfficientNet/ResNet),
o	A bottleneck (here, replaced by a Transformer for global context),
o	And an expanding path (the decoder).
•	The symmetrical design of U-Net is preserved in structure.

2. Fusion Layer or Connections
•	U-Net uses direct fusion of layers from the corresponding convolution between encoder and decoder layers to recover spatial detail.
•	In MAHT-Net, these are replaced/enhanced by:
o	Multi-Scale Feature Fusion (like in FPN),
o	And Attention Gates, which act like intelligent skip connections, allowing only relevant features through.

3. Upsampling Decoder
•	U-Net’s decoder uses upsampling and convolution to restore image resolution.
•	MAHT-Net does the same but with added attention modules and multi-scale fusion, improving upon U-Net’s naive upsampling.

4. Fully Convolutional & End-to-End
•	Both architectures are fully convolutional, accept full images, and output either a mask or heatmap in the same spatial resolution.

Advantages MAHT-Net over U-Net
Feature	U-Net	MAHT-Net (Proposed)
Global Context Understanding	NO	YES - Transformer Bottleneck
Focused Attention	NO	YES - Attention Gates
Robust to Scale Variation	NO	YES - Multi-Scale Fusion (FPN)
Landmark Heatmap Regression	 (coordinate)	YES - (Gaussian heatmap output)
Interpretability	Basic	High (attention maps, uncertainty)





